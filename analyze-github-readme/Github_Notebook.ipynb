{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is your Github Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Github Scraping\n",
    "For collecting the initial dataset using the Github API. Collect up to 1000 repo readme's for the top 10 programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from github3 import login, GitHub\n",
    "from github3.null import NullObject\n",
    "from getpass import getpass, getuser\n",
    "import os.path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "def saveResults(fileName, jsonObject):\n",
    "    f = open(fileName, \"w\")\n",
    "    f.write(json.dumps(jsonObject))\n",
    "    f.close()\n",
    "\n",
    "def checkResults(fileName):\n",
    "    return os.path.isfile(fileName)\n",
    "\n",
    "try:\n",
    "    import readline\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    user = raw_input('GitHub username: ')\n",
    "except KeyboardInterrupt:\n",
    "    user = getuser()\n",
    "\n",
    "password = getpass('GitHub password for {0}: '.format(user))\n",
    "\n",
    "# Obviously you could also prompt for an OAuth token\n",
    "if not (user and password):\n",
    "    print(\"Cowardly refusing to login without a username and password.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "g = login(user, password)\n",
    "\n",
    "# ~960k repos\n",
    "# languages: javscript, java, ruby, python, php, html, css, c++, c, c#\n",
    "languages = [\"javascript\", \"java\", \"ruby\", \"python\", \"php\", \"html\", \"css\", \"c++\", \"c\", \"c#\"]\n",
    "\n",
    "numRepos = 1000\n",
    "\n",
    "results = {}\n",
    "\n",
    "lCount = 0\n",
    "rCount = 0\n",
    "for language in languages:\n",
    "    r = g.search_repositories(\"language:\" + language, sort=\"stars\", number=numRepos)\n",
    "    results[language] = []\n",
    "    if checkResults(\"results-\" + language + \".json\"):\n",
    "        continue\n",
    "    while g.rate_limit()[\"rate\"][\"remaining\"] < numRepos:\n",
    "        print(\"API timeout reached. Waiting for 60 seconds.\")\n",
    "        time.sleep(60)\n",
    "    for i in r:\n",
    "        repo = i.repository\n",
    "        readme = repo.readme()\n",
    "        text = readme.decoded\n",
    "        extension = readme.html_url.split(\".\")[-1].lower()\n",
    "        numStars = repo.stargazers_count\n",
    "        numForks = repo.forks_count\n",
    "        numWatches = repo.watchers\n",
    "        if type(readme) == NullObject:\n",
    "            # Skip repos that don't have a readme\n",
    "            pass\n",
    "        else:\n",
    "            jsonObject = {\n",
    "                \"text\": text,\n",
    "                \"extension\": extension,\n",
    "                \"numStars\": numStars,\n",
    "                \"numForks\": numForks,\n",
    "                \"numWatches\": numWatches\n",
    "            }\n",
    "            results[language].append(jsonObject)\n",
    "        print str(lCount) + \"/\" + str(rCount)\n",
    "        rCount += 1\n",
    "    saveResults(\"results-\" + language + \".json\", {language: results[language]})\n",
    "    lCount += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction\n",
    "Here we extract the features from each individual github repository. We collect 5 different feautures:\n",
    "* Header Tags (h1, h2 & h3)\n",
    "* Paragraph Tags (p)\n",
    "* \\# of code snippets (pre)\n",
    "* \\# of images & badges (img)\n",
    "* Length of readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import mistune\n",
    "import string\n",
    "from docutils import core\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def saveResults(fileName, jsonObject):\n",
    "    f = open(fileName, \"w\")\n",
    "    f.write(json.dumps(jsonObject))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def loadResults(fileName):\n",
    "    f = open(fileName, \"r+\")\n",
    "    return json.loads(f.read())\n",
    "\n",
    "\n",
    "def html_parts(input_string, source_path=None, destination_path=None,\n",
    "               input_encoding='unicode', doctitle=True,\n",
    "               initial_header_level=1):\n",
    "    \"\"\"\n",
    "    Given an input string, returns a dictionary of HTML document parts.\n",
    "\n",
    "    Dictionary keys are the names of parts, and values are Unicode strings;\n",
    "    encoding is up to the client.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    - `input_string`: A multi-line text string; required.\n",
    "    - `source_path`: Path to the source file or object.  Optional, but useful\n",
    "      for diagnostic output (system messages).\n",
    "    - `destination_path`: Path to the file or object which will receive the\n",
    "      output; optional.  Used for determining relative paths (stylesheets,\n",
    "      source links, etc.).\n",
    "    - `input_encoding`: The encoding of `input_string`.  If it is an encoded\n",
    "      8-bit string, provide the correct encoding.  If it is a Unicode string,\n",
    "      use \"unicode\", the default.\n",
    "    - `doctitle`: Disable the promotion of a lone top-level section title to\n",
    "      document title (and subsequent section title to document subtitle\n",
    "      promotion); enabled by default.\n",
    "    - `initial_header_level`: The initial level for header elements (e.g. 1\n",
    "      for \"<h1>\").\n",
    "    \"\"\"\n",
    "    overrides = {'input_encoding': input_encoding,\n",
    "                 'doctitle_xform': doctitle,\n",
    "                 'initial_header_level': initial_header_level}\n",
    "    parts = core.publish_parts(\n",
    "        source=input_string, source_path=source_path,\n",
    "        destination_path=destination_path,\n",
    "        writer_name='html', settings_overrides=overrides)\n",
    "    return parts\n",
    "\n",
    "\n",
    "languages = [\"javascript\", \"java\", \"ruby\", \"python\", \"php\", \"html\", \"css\", \"c++\", \"c\", \"c#\"]\n",
    "\n",
    "def parseMarkdown(md):\n",
    "    '''\n",
    "    Returns HTML\n",
    "    '''\n",
    "    return mistune.markdown(md)\n",
    "\n",
    "\n",
    "def parseRestructuredtext(rst):\n",
    "    '''\n",
    "    Returns HTML\n",
    "    '''\n",
    "    return html_parts(rst)[\"whole\"]\n",
    "\n",
    "\n",
    "def parseHTML(html):\n",
    "    '''\n",
    "    Returns Plain Text\n",
    "    '''\n",
    "    return BeautifulSoup(html).get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "total_stats = {}\n",
    "h_X = {}\n",
    "h_y = {}\n",
    "p_X = {}\n",
    "p_y = {}\n",
    "i_X = {}\n",
    "i_y = {}\n",
    "pr_X = {}\n",
    "pr_y = {}\n",
    "l_X = {}\n",
    "l_y = {}\n",
    "ss = SnowballStemmer(\"english\")\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "h_X[\"all\"] = []\n",
    "h_y[\"all\"] = []\n",
    "p_X[\"all\"] = []\n",
    "p_y[\"all\"] = []\n",
    "i_X[\"all\"] = []\n",
    "i_y[\"all\"] = []\n",
    "pr_X[\"all\"] = []\n",
    "pr_y[\"all\"] = []\n",
    "l_X[\"all\"] = []\n",
    "l_y[\"all\"] = []\n",
    "\n",
    "total_stats[\"all\"] = []\n",
    "\n",
    "# data preprocessing\n",
    "for lang in languages:\n",
    "    total_stats[lang] = []\n",
    "    results = loadResults(\"results-\" + lang + \".json\")[lang]\n",
    "    h_X[lang] = []\n",
    "    h_y[lang] = []\n",
    "    p_X[lang] = []\n",
    "    p_y[lang] = []\n",
    "    i_X[lang] = []\n",
    "    i_y[lang] = []\n",
    "    pr_X[lang] = []\n",
    "    pr_y[lang] = []\n",
    "    l_X[lang] = []\n",
    "    l_y[lang] = []\n",
    "    \n",
    "    ground_truth = 999\n",
    "    label = -1\n",
    "    \n",
    "    for i in results:\n",
    "        # try to parse the data with best effort\n",
    "        try:\n",
    "            if i[\"extension\"] == \"md\":\n",
    "                html = parseMarkdown(i[\"text\"])\n",
    "            elif i[\"extension\"] == \"rst\":\n",
    "                html = parseRestructuredtext(i[\"text\"])\n",
    "            else:\n",
    "                continue\n",
    "            bs = BeautifulSoup(html, \"lxml\")\n",
    "            \n",
    "            label = ground_truth / 100\n",
    "            \n",
    "            \n",
    "            # collect stats here\n",
    "            stats = {}\n",
    "            stats[\"numPre\"] = len(bs.find_all(\"pre\"))\n",
    "            stats[\"numImg\"] = len(bs.find_all(\"img\"))\n",
    "            stats[\"totalLength\"] = len(bs.get_text())\n",
    "            stats[\"numStars\"] = i[\"numStars\"]\n",
    "            stats[\"numForks\"] = i[\"numForks\"]\n",
    "            stats[\"numWatches\"] = i[\"numWatches\"]\n",
    "            pass\n",
    "            i_X[lang].append(stats[\"numImg\"])\n",
    "            i_X[\"all\"].append(stats[\"numImg\"])\n",
    "            #i_y[lang].append(stats[\"numStars\"])\n",
    "            i_y[lang].append(label)\n",
    "            i_y[\"all\"].append(label)\n",
    "            pr_X[lang].append(stats[\"numPre\"])\n",
    "            pr_X[\"all\"].append(stats[\"numPre\"])\n",
    "            #pr_y[lang].append(stats[\"numStars\"])\n",
    "            pr_y[lang].append(label)\n",
    "            pr_y[\"all\"].append(label)\n",
    "            l_X[lang].append(stats[\"totalLength\"])\n",
    "            l_X[\"all\"].append(stats[\"totalLength\"])\n",
    "            #l_y[lang].append(stats[\"numStars\"])\n",
    "            l_y[lang].append(label)\n",
    "            l_y[\"all\"].append(label)\n",
    "            # remove new lines, remove punctuation and stem words\n",
    "            headlines = [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"h1\")]\n",
    "            headlines += [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"h2\")]\n",
    "            headlines += [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"h3\")]\n",
    "            headlines = [\" \".join([ss.stem(j.lower()) for j in i.split()]) for i in headlines]\n",
    "            paragraphs = [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"p\")]\n",
    "            paragraphs = [\" \".join([ss.stem(j.lower()) for j in i.split()]) for i in paragraphs]\n",
    "            \n",
    "            # concatenate strings\n",
    "            headlines = \" \".join(headlines)\n",
    "            h_X[lang].append(headlines)\n",
    "            h_X[\"all\"].append(headlines)\n",
    "            #h_y[lang].append(stats[\"numStars\"])\n",
    "            h_y[lang].append(label)\n",
    "            h_y[\"all\"].append(label)\n",
    "            \n",
    "            paragraphs = \" \".join(paragraphs)\n",
    "            p_X[lang].append(paragraphs)\n",
    "            p_X[\"all\"].append(paragraphs)\n",
    "            #p_y[lang].append(stats[\"numStars\"])\n",
    "            p_y[lang].append(label)\n",
    "            p_y[\"all\"].append(label)\n",
    "            \n",
    "            #for i in headlines:\n",
    "            #    if len(i) != 0:\n",
    "            #        h_X[lang].append(i)\n",
    "            #        h_y[lang].append(stats[\"numStars\"])\n",
    "            #\n",
    "            #for i in paragraphs:\n",
    "            #    if len(i) != 0:\n",
    "            #        p_X[lang].append(i)\n",
    "            #        p_y[lang].append(stats[\"numStars\"])\n",
    "            #\n",
    "            total_stats[lang].append(stats)\n",
    "            total_stats[\"all\"].append(stats)\n",
    "            \n",
    "            # decrease ground truth score\n",
    "            ground_truth -= 1\n",
    "        except ValueError as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model selection\n",
    "\n",
    "## 3.1. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\",\"about\",\"above\",\"across\",\"after\",\"again\",\"against\",\"all\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\n",
    "              \"although\",\"always\",\"among\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyone\",\"anything\",\"anywhere\",\"are\",\n",
    "              \"area\",\"areas\",\"around\",\"as\",\"ask\",\"asked\",\"asking\",\"asks\",\"at\",\"away\",\"b\",\"back\",\"backed\",\"backing\",\n",
    "              \"backs\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"been\",\"before\",\"began\",\"behind\",\"being\",\"beings\",\n",
    "              \"best\",\"better\",\"between\",\"big\",\"both\",\"but\",\"by\",\"c\",\"came\",\"can\",\"cannot\",\"case\",\"cases\",\"certain\",\n",
    "              \"certainly\",\"clear\",\"clearly\",\"come\",\"could\",\"d\",\"did\",\"differ\",\"different\",\"differently\",\"do\",\"does\",\n",
    "              \"done\",\"down\",\"down\",\"downed\",\"downing\",\"downs\",\"during\",\"e\",\"each\",\"early\",\"either\",\"end\",\"ended\",\n",
    "              \"ending\",\"ends\",\"enough\",\"even\",\"evenly\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\n",
    "              \"f\",\"face\",\"faces\",\"fact\",\"facts\",\"far\",\"felt\",\"few\",\"find\",\"finds\",\"first\",\"for\",\"four\",\"from\",\"full\",\n",
    "              \"fully\",\"further\",\"furthered\",\"furthering\",\"furthers\",\"g\",\"gave\",\"general\",\"generally\",\"get\",\"gets\",\n",
    "              \"give\",\"given\",\"gives\",\"go\",\"going\",\"good\",\"goods\",\"got\",\"great\",\"greater\",\"greatest\",\"group\",\"grouped\",\n",
    "              \"grouping\",\"groups\",\"h\",\"had\",\"has\",\"have\",\"having\",\"he\",\"her\",\"here\",\"herself\",\"high\",\"high\",\"high\",\n",
    "              \"higher\",\"highest\",\"him\",\"himself\",\"his\",\"how\",\"however\",\"i\",\"if\",\"important\",\"in\",\"interest\",\n",
    "              \"interested\",\"interesting\",\"interests\",\"into\",\"is\",\"it\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\n",
    "              \"kind\",\"knew\",\"know\",\"known\",\"knows\",\"l\",\"large\",\"largely\",\"last\",\"later\",\"latest\",\"least\",\"less\",\"let\",\n",
    "              \"lets\",\"like\",\"likely\",\"long\",\"longer\",\"longest\",\"m\",\"made\",\"make\",\"making\",\"man\",\"many\",\"may\",\"me\",\n",
    "              \"member\",\"members\",\"men\",\"might\",\"more\",\"most\",\"mostly\",\"mr\",\"mrs\",\"much\",\"must\",\"my\",\"myself\",\"n\",\n",
    "              \"necessary\",\"need\",\"needed\",\"needing\",\"needs\",\"never\",\"new\",\"new\",\"newer\",\"newest\",\"next\",\"no\",\"nobody\",\n",
    "              \"non\",\"noone\",\"not\",\"nothing\",\"now\",\"nowhere\",\"number\",\"numbers\",\"o\",\"of\",\"off\",\"often\",\"old\",\"older\",\n",
    "              \"oldest\",\"on\",\"once\",\"one\",\"only\",\"open\",\"opened\",\"opening\",\"opens\",\"or\",\"order\",\"ordered\",\"ordering\",\n",
    "              \"orders\",\"other\",\"others\",\"our\",\"out\",\"over\",\"p\",\"part\",\"parted\",\"parting\",\"parts\",\"per\",\"perhaps\",\n",
    "              \"place\",\"places\",\"point\",\"pointed\",\"pointing\",\"points\",\"possible\",\"present\",\"presented\",\"presenting\",\n",
    "              \"presents\",\"problem\",\"problems\",\"put\",\"puts\",\"q\",\"quite\",\"r\",\"rather\",\"really\",\"right\",\"right\",\"room\",\n",
    "              \"rooms\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"says\",\"second\",\"seconds\",\"see\",\"seem\",\"seemed\",\"seeming\",\"seems\",\n",
    "              \"sees\",\"several\",\"shall\",\"she\",\"should\",\"show\",\"showed\",\"showing\",\"shows\",\"side\",\"sides\",\"since\",\n",
    "              \"small\",\"smaller\",\"smallest\",\"so\",\"some\",\"somebody\",\"someone\",\"something\",\"somewhere\",\"state\",\"states\",\n",
    "              \"still\",\"still\",\"such\",\"sure\",\"t\",\"take\",\"taken\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\n",
    "              \"therefore\",\"these\",\"they\",\"thing\",\"things\",\"think\",\"thinks\",\"this\",\"those\",\"though\",\"thought\",\n",
    "              \"thoughts\",\"three\",\"through\",\"thus\",\"to\",\"today\",\"together\",\"too\",\"took\",\"toward\",\"turn\",\"turned\",\n",
    "              \"turning\",\"turns\",\"two\",\"u\",\"under\",\"until\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"uses\",\"v\",\"very\",\"w\",\"want\",\n",
    "              \"wanted\",\"wanting\",\"wants\",\"was\",\"way\",\"ways\",\"we\",\"well\",\"wells\",\"went\",\"were\",\"what\",\"when\",\"where\",\n",
    "              \"whether\",\"which\",\"while\",\"who\",\"whole\",\"whose\",\"why\",\"will\",\"with\",\"within\",\"without\",\"work\",\"worked\",\n",
    "              \"working\",\"works\",\"would\",\"x\",\"y\",\"year\",\"years\",\"yet\",\"you\",\"young\",\"younger\",\"youngest\",\"your\",\n",
    "              \"yours\",\"z\"]\n",
    "\n",
    "ss = SnowballStemmer(\"english\")\n",
    "\n",
    "stop_words = list(set([ss.stem(i) for i in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from numpy import array\n",
    "\n",
    "#scaler = MinMaxScaler(feature_range=(0.5, 1.0))\n",
    "\n",
    "prepared_data = {}\n",
    "\n",
    "languages.append(\"all\")\n",
    "\n",
    "for lang in languages:\n",
    "    prepared_data[lang] = {}\n",
    "    \n",
    "    # Initialize vectorizers\n",
    "    h_tfidf = TfidfVectorizer(min_df = 0.04, max_df = 0.96, stop_words=stop_words)\n",
    "    p_tfidf = TfidfVectorizer(min_df = 0.04, max_df = 0.96, stop_words=stop_words)\n",
    "    #h_v1g = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    #p_v1g = CountVectorizer(ngram_range=(1, 1), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    #v4g = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    #v3g = CountVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    \n",
    "    ## Header\n",
    "    X_n = h_tfidf.fit_transform(h_X[lang])\n",
    "    y_s = array(h_y[lang])\n",
    "\n",
    "    X_n_train, X_n_test, y_s_train, y_s_test = train_test_split(X_n, y_s, test_size=0.25, random_state=42)\n",
    "    \n",
    "    prepared_data[lang][\"header\"] = {}\n",
    "    prepared_data[lang][\"header\"][\"X_n\"] = X_n\n",
    "    prepared_data[lang][\"header\"][\"y_s\"] = y_s\n",
    "    prepared_data[lang][\"header\"][\"X_n_train\"] = X_n_train\n",
    "    prepared_data[lang][\"header\"][\"X_n_test\"] = X_n_test\n",
    "    prepared_data[lang][\"header\"][\"y_s_train\"] = y_s_train\n",
    "    prepared_data[lang][\"header\"][\"y_s_test\"] = y_s_test\n",
    "    prepared_data[lang][\"header\"][\"v1g\"] = h_tfidf\n",
    "    \n",
    "    ## Paragraph\n",
    "    X_n = p_tfidf.fit_transform(p_X[lang])\n",
    "    y_s = array(p_y[lang])\n",
    "\n",
    "    X_n_train, X_n_test, y_s_train, y_s_test = train_test_split(X_n, y_s, test_size=0.25, random_state=42)\n",
    "    \n",
    "    prepared_data[lang][\"paragraph\"] = {}\n",
    "    prepared_data[lang][\"paragraph\"][\"X_n\"] = X_n\n",
    "    prepared_data[lang][\"paragraph\"][\"y_s\"] = y_s\n",
    "    prepared_data[lang][\"paragraph\"][\"X_n_train\"] = X_n_train\n",
    "    prepared_data[lang][\"paragraph\"][\"X_n_test\"] = X_n_test\n",
    "    prepared_data[lang][\"paragraph\"][\"y_s_train\"] = y_s_train\n",
    "    prepared_data[lang][\"paragraph\"][\"y_s_test\"] = y_s_test\n",
    "    prepared_data[lang][\"paragraph\"][\"v1g\"] = p_tfidf\n",
    "    \n",
    "    # Number of Pre tags\n",
    "    X_n = array(pr_X[lang])\n",
    "    y_s = array(pr_y[lang])\n",
    "\n",
    "    X_n_train, X_n_test, y_s_train, y_s_test = train_test_split(X_n, y_s, test_size=0.25, random_state=42)\n",
    "    \n",
    "    prepared_data[lang][\"pre\"] = {}\n",
    "    prepared_data[lang][\"pre\"][\"X_n\"] = X_n\n",
    "    prepared_data[lang][\"pre\"][\"y_s\"] = y_s\n",
    "    prepared_data[lang][\"pre\"][\"X_n_train\"] = X_n_train\n",
    "    prepared_data[lang][\"pre\"][\"X_n_test\"] = X_n_test\n",
    "    prepared_data[lang][\"pre\"][\"y_s_train\"] = y_s_train\n",
    "    prepared_data[lang][\"pre\"][\"y_s_test\"] = y_s_test\n",
    "    \n",
    "    # Number of Img tags\n",
    "    X_n = array(i_X[lang])\n",
    "    y_s = array(i_y[lang])\n",
    "\n",
    "    X_n_train, X_n_test, y_s_train, y_s_test = train_test_split(X_n, y_s, test_size=0.25, random_state=42)\n",
    "    \n",
    "    prepared_data[lang][\"img\"] = {}\n",
    "    prepared_data[lang][\"img\"][\"X_n\"] = X_n\n",
    "    prepared_data[lang][\"img\"][\"y_s\"] = y_s\n",
    "    prepared_data[lang][\"img\"][\"X_n_train\"] = X_n_train\n",
    "    prepared_data[lang][\"img\"][\"X_n_test\"] = X_n_test\n",
    "    prepared_data[lang][\"img\"][\"y_s_train\"] = y_s_train\n",
    "    prepared_data[lang][\"img\"][\"y_s_test\"] = y_s_test\n",
    "    \n",
    "    # Readme length\n",
    "    X_n = array(l_X[lang])\n",
    "    y_s = array(l_y[lang])\n",
    "\n",
    "    X_n_train, X_n_test, y_s_train, y_s_test = train_test_split(X_n, y_s, test_size=0.25, random_state=42)\n",
    "    \n",
    "    prepared_data[lang][\"length\"] = {}\n",
    "    prepared_data[lang][\"length\"][\"X_n\"] = X_n\n",
    "    prepared_data[lang][\"length\"][\"y_s\"] = y_s\n",
    "    prepared_data[lang][\"length\"][\"X_n_train\"] = X_n_train\n",
    "    prepared_data[lang][\"length\"][\"X_n_test\"] = X_n_test\n",
    "    prepared_data[lang][\"length\"][\"y_s_train\"] = y_s_train\n",
    "    prepared_data[lang][\"length\"][\"y_s_test\"] = y_s_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Benchmarking\n",
    "Our whole assumption is based on that the top 1000 repositories for each language are the best type of readmes a developer can write.\n",
    "\n",
    "nuSVM was removed because it has no direct interpretation. (http://stackoverflow.com/questions/11230955/what-is-the-meaning-of-the-nu-parameter-in-scikit-learns-svm-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from numpy import ndarray\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "models = {}\n",
    "scores = {}\n",
    "timer = {}\n",
    "\n",
    "features = [\"header\", \"paragraph\", \"pre\", \"img\", \"length\"]\n",
    "\n",
    "def init_classifiers():\n",
    "    classifiers = {}\n",
    "    classifiers[\"LogisticRegression\"] = {\"model\": LogisticRegression()}\n",
    "    classifiers[\"PassiveAggressiveClassifier\"] = {\"model\": PassiveAggressiveClassifier()}\n",
    "    classifiers[\"GaussianNB\"] = {\"model\": GaussianNB()}\n",
    "    classifiers[\"MultinomialNB\"] = {\"model\": MultinomialNB()}\n",
    "    classifiers[\"BernoulliNB\"] = {\"model\": BernoulliNB()}\n",
    "    classifiers[\"NearestCentroid\"] = {\"model\": NearestCentroid()}\n",
    "    classifiers[\"LabelPropagation\"] = {\"model\": LabelPropagation()}\n",
    "    classifiers[\"SVC\"] = {\"model\": SVC()}\n",
    "    classifiers[\"LinearSVC\"] = {\"model\": LinearSVC()}\n",
    "    classifiers[\"DecisionTreeClassifier\"] = {\"model\": DecisionTreeClassifier()}\n",
    "    classifiers[\"ExtraTreeClassifier\"] = {\"model\": ExtraTreeClassifier()}\n",
    "    \n",
    "    return classifiers\n",
    "c = 0\n",
    "for lang in languages:\n",
    "    models[lang] = {}\n",
    "    scores[lang] = {}\n",
    "    timer[lang] = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        classifiers = init_classifiers()\n",
    "        models[lang][feature] = {}\n",
    "        scores[lang][feature] = {}\n",
    "        timer[lang][feature] = {}\n",
    "        \n",
    "        for name in classifiers:\n",
    "            clf = classifiers[name][\"model\"]\n",
    "            X = prepared_data[lang][feature][\"X_n_train\"]\n",
    "            y = prepared_data[lang][feature][\"y_s_train\"]\n",
    "            X_test = prepared_data[lang][feature][\"X_n_test\"]\n",
    "            y_test = prepared_data[lang][feature][\"y_s_test\"]\n",
    "            if feature in [\"pre\", \"img\", \"length\"]:\n",
    "                X = X.reshape(-1,1)\n",
    "                X_test = X_test.reshape(-1,1)\n",
    "            # start timing\n",
    "            if name in [\"LabelPropagation\", \"GaussianNB\"]:\n",
    "                X = X.toarray() if not isinstance(X, ndarray) else X\n",
    "                X_test = X_test.toarray() if not isinstance(X_test, ndarray) else X_test\n",
    "            print \"Language: \" + lang\n",
    "            print \"Classifier: \" + name\n",
    "            print \"Feature: \" + feature\n",
    "            print \"=================================\"\n",
    "            start = timeit.default_timer()\n",
    "            clf.fit(X, y)\n",
    "            # end timing\n",
    "            stop = timeit.default_timer()\n",
    "            timer[lang][feature][name] = stop - start\n",
    "            \n",
    "            predictions = clf.predict(X_test)\n",
    "            true_values = y_test\n",
    "            classifiers[name][\"predictor\"] = clf\n",
    "            scores[lang][feature][name] = mean_squared_error(predictions, true_values)\n",
    "            \n",
    "        # Get the best model\n",
    "        models[lang][feature][\"model\"] = classifiers[min(scores[lang][feature], key=scores[lang][feature].get)][\"predictor\"]\n",
    "        models[lang][feature][\"type\"] = min(scores[lang][feature], key=scores[lang][feature].get)\n",
    "        # add v1g objects to the models object\n",
    "        if feature in [\"header\", \"paragraph\"]:\n",
    "            models[lang][feature][\"v1g\"] = prepared_data[lang][feature][\"v1g\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# persist model file\n",
    "output = open(\"models.pkl\", \"wb\")\n",
    "\n",
    "pickle.dump(models, output)\n",
    "output.close()\n",
    "\n",
    "#output2 = open(\"scores.pkl\", \"wb\")\n",
    "\n",
    "#pickle.dump(scores, output2)\n",
    "#output2.close()\n",
    "\n",
    "#output3 = open(\"timer.pkl\", \"wb\")\n",
    "\n",
    "#pickle.dump(timer, output3)\n",
    "#output3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scoring Github Readmes\n",
    "First pull the file from the given url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import zipfile\n",
    "\n",
    "# load zipped models file into memory\n",
    "zip_ref = zipfile.ZipFile(\"models.pkl.zip\", 'r')\n",
    "zip_ref.extract(\"models.pkl\")\n",
    "zip_ref.close()\n",
    "\n",
    "# load model file into memory\n",
    "file_handle = open(\"models.pkl\", \"rb\")\n",
    "\n",
    "models = pickle.load(file_handle)\n",
    "\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mistune\n",
    "import urllib\n",
    "import string\n",
    "from numpy import array\n",
    "from docutils import core\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def html_parts(input_string, source_path=None, destination_path=None,\n",
    "               input_encoding='unicode', doctitle=True,\n",
    "               initial_header_level=1):\n",
    "    \"\"\"\n",
    "    Given an input string, returns a dictionary of HTML document parts.\n",
    "\n",
    "    Dictionary keys are the names of parts, and values are Unicode strings;\n",
    "    encoding is up to the client.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    - `input_string`: A multi-line text string; required.\n",
    "    - `source_path`: Path to the source file or object.  Optional, but useful\n",
    "      for diagnostic output (system messages).\n",
    "    - `destination_path`: Path to the file or object which will receive the\n",
    "      output; optional.  Used for determining relative paths (stylesheets,\n",
    "      source links, etc.).\n",
    "    - `input_encoding`: The encoding of `input_string`.  If it is an encoded\n",
    "      8-bit string, provide the correct encoding.  If it is a Unicode string,\n",
    "      use \"unicode\", the default.\n",
    "    - `doctitle`: Disable the promotion of a lone top-level section title to\n",
    "      document title (and subsequent section title to document subtitle\n",
    "      promotion); enabled by default.\n",
    "    - `initial_header_level`: The initial level for header elements (e.g. 1\n",
    "      for \"<h1>\").\n",
    "    \"\"\"\n",
    "    overrides = {'input_encoding': input_encoding,\n",
    "                 'doctitle_xform': doctitle,\n",
    "                 'initial_header_level': initial_header_level}\n",
    "    parts = core.publish_parts(\n",
    "        source=input_string, source_path=source_path,\n",
    "        destination_path=destination_path,\n",
    "        writer_name='html', settings_overrides=overrides)\n",
    "    return parts\n",
    "\n",
    "def parseMarkdown(md):\n",
    "    '''\n",
    "    Returns HTML\n",
    "    '''\n",
    "    return mistune.markdown(md)\n",
    "\n",
    "\n",
    "def parseRestructuredtext(rst):\n",
    "    '''\n",
    "    Returns HTML\n",
    "    '''\n",
    "    return html_parts(rst)[\"whole\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_readme_url = \"https://raw.githubusercontent.com/algorithmiaio/algorithmia-python/master/README.md\"\n",
    "\n",
    "languages = [\"javascript\", \"java\", \"ruby\", \"python\", \"php\", \"html\", \"css\", \"c++\", \"c\", \"c#\", \"all\"]\n",
    "\n",
    "given_language = \"all\"\n",
    "\n",
    "if given_language.lower() not in languages:\n",
    "    raise ValueError(\"Language (\" + given_language + \") is not supported\")\n",
    "\n",
    "file_format = example_readme_url.split(\".\")[-1].lower()\n",
    "\n",
    "given_readme = urllib.urlopen(example_readme_url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = {}\n",
    "\n",
    "vectorizer[\"header\"] = models[given_language.lower()][\"header\"][\"v1g\"]\n",
    "vectorizer[\"paragraph\"] = models[given_language.lower()][\"paragraph\"][\"v1g\"]\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "ss = SnowballStemmer(\"english\")\n",
    "\n",
    "if file_format == \"md\":\n",
    "    html = parseMarkdown(given_readme)\n",
    "elif file_format == \"rst\":\n",
    "    html = parseRestructuredtext(given_readme.decode(\"utf-8\"))\n",
    "\n",
    "bs = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# remove new lines, remove punctuation and stem words\n",
    "headlines = [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"h1\")]\n",
    "headlines += [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"h2\")]\n",
    "headlines += [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"h3\")]\n",
    "headlines = [\" \".join([ss.stem(j.lower()) for j in i.split()]) for i in headlines]\n",
    "headlines = [\" \".join(headlines)]\n",
    "paragraphs = [i.get_text().replace(\"\\n\", \" \").translate(translate_table) for i in bs.find_all(\"p\")]\n",
    "paragraphs = [\" \".join([ss.stem(j.lower()) for j in i.split()]) for i in paragraphs]\n",
    "paragraphs = [\" \".join(paragraphs)]\n",
    "\n",
    "input_vector = {}\n",
    "\n",
    "input_vector[\"img\"] = array(len(bs.find_all(\"img\")))\n",
    "input_vector[\"pre\"] = array(len(bs.find_all(\"pre\")))\n",
    "input_vector[\"length\"] = len(bs.get_text())\n",
    "input_vector[\"header\"] = vectorizer[\"header\"].transform(headlines)\n",
    "input_vector[\"paragraph\"] = vectorizer[\"paragraph\"].transform(paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get scores for each features\n",
    "standard_results = {}\n",
    "\n",
    "standard_results[\"header\"] = int(models[given_language.lower()][\"header\"][\"model\"].predict(input_vector[\"header\"])[0])\n",
    "standard_results[\"paragraph\"] = int(models[given_language.lower()][\"paragraph\"][\"model\"].predict(input_vector[\"paragraph\"])[0])\n",
    "standard_results[\"pre\"] = int(models[given_language.lower()][\"pre\"][\"model\"].predict(input_vector[\"pre\"])[0])\n",
    "standard_results[\"img\"] = int(models[given_language.lower()][\"img\"][\"model\"].predict(input_vector[\"img\"])[0])\n",
    "standard_results[\"length\"] = int(models[given_language.lower()][\"length\"][\"model\"].predict(input_vector[\"length\"])[0])\n",
    "\n",
    "print standard_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Improve score by flipping binary words in feature vectors\n",
    "\n",
    "features = [\"header\", \"paragraph\", \"pre\", \"img\", \"length\"]\n",
    "\n",
    "\n",
    "improved_results = {}\n",
    "\n",
    "for feature in features:\n",
    "    improved_results[feature] = []\n",
    "    # Find a better local minima for the feature\n",
    "    X = input_vector[feature]\n",
    "    # works only for text\n",
    "    if feature in [\"header\", \"paragraph\"]:\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[0, j] == 0:\n",
    "                X[0, j] = 1\n",
    "                res = models[given_language.lower()][feature][\"model\"].predict(X)\n",
    "                res = int(res[0])\n",
    "                improved_results[feature].append(res)\n",
    "                X[0, j] = 0\n",
    "            elif X[0, j] == 1:\n",
    "                X[0, j] = 0\n",
    "                res = models[given_language.lower()][feature][\"model\"].predict(X)\n",
    "                res = int(res[0])\n",
    "                improved_results[feature].append(res)\n",
    "                X[0, j] = 1\n",
    "    elif feature in [\"pre\", \"img\"]:\n",
    "        # decrease length in single increments from %200 to %0\n",
    "        if int(X) == 0:\n",
    "            pass\n",
    "        elif int(X) > 0:\n",
    "            # down to %0\n",
    "            for i in range(int(X)):\n",
    "                new_val = array(int(X) - i - 1)\n",
    "                res = models[given_language.lower()][feature][\"model\"].predict(new_val)\n",
    "                improved_results[feature].append(res)\n",
    "            # up to %200\n",
    "            for i in range(2 * int(X)):\n",
    "                new_val = array(int(X) + i + 1)\n",
    "                res = models[given_language.lower()][feature][\"model\"].predict(new_val)\n",
    "                improved_results[feature].append(res)\n",
    "    elif feature in [\"length\"]:\n",
    "        # increase & decrease length in %10 increments from %0 to %200\n",
    "        if int(X) > 20:\n",
    "            for i in range(21):\n",
    "                new_val = array(int(int(X) * 0.1 * (20 - i)))\n",
    "                res = models[given_language.lower()][feature][\"model\"].predict(new_val)\n",
    "                improved_results[feature].append(res)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import operator\n",
    "\n",
    "changes = {}\n",
    "\n",
    "for feature in features:\n",
    "    if feature in [\"header\", \"paragraph\"]:\n",
    "        top_n_changes = heapq.nlargest(10, enumerate(improved_results[feature]), key=operator.itemgetter(1))\n",
    "\n",
    "        indel = None\n",
    "        \n",
    "        changes[feature] = []\n",
    "\n",
    "        for i in top_n_changes:\n",
    "            if vectorizer[feature].vocabulary_.keys()[i[0]] in headlines[0].split():\n",
    "                indel = \"delete\"\n",
    "            elif vectorizer[feature].vocabulary_.keys()[i[0]] not in headlines[0].split():\n",
    "                indel = \"insert\"\n",
    "            else:\n",
    "                indel = None\n",
    "            # only show changes that are useful\n",
    "            if i[1] >= standard_results[feature]:\n",
    "                changes[feature].append({\"value\": vectorizer[feature].vocabulary_.keys()[i[0]], \"operation\": indel})\n",
    "    elif feature in [\"pre\", \"img\", \"length\"]:\n",
    "        top_n_changes = heapq.nlargest(10, enumerate(improved_results[feature]), key=operator.itemgetter(1))\n",
    "        \n",
    "        oper = None\n",
    "        \n",
    "        changes[feature] = []\n",
    "        \n",
    "        if len(top_n_changes) > 0:\n",
    "            i = top_n_changes[0]\n",
    "            if i[0] > input_vector[feature]:\n",
    "                oper = \"increase\"\n",
    "            elif i[0] < input_vector[feature]:\n",
    "                oper = \"decrease\"\n",
    "            if i[1] >= standard_results[feature]:\n",
    "                changes[feature].append({\"value\": i[0], \"operation\": oper})\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in top_n_changes:\n",
    "            if i[0] > input_vector[feature]:\n",
    "                oper = \"addition\"\n",
    "            elif i[0] < input_vector[feature]:\n",
    "                oper = \"subtraction\"\n",
    "            if i[1] >= standard_results[feature]:\n",
    "                changes[feature].append({\"value\": i[0], \"operation\": oper})\n",
    "        \"\"\"\n",
    "        \n",
    "print changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
